{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e028f39-95ea-4388-8cfb-3d3775613809",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install dqx"
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-labs-dqx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7d5b86c-8692-4f50-a612-32f01a9f32eb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Restart Python"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1762eb6f-7149-4d0f-89b0-c49d8630edca",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get bronze dataframe"
    }
   },
   "outputs": [],
   "source": [
    "raw_matches_df = spark.table('default.raw_matches')\n",
    "display(raw_matches_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eecd0374-ae9f-47ce-999d-457268fcc90e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define base transformations"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, explode, get\n",
    "from pyspark.sql.functions import to_date, to_timestamp, col\n",
    "\n",
    "def explode_arr(df: DataFrame) -> DataFrame:\n",
    "    return df.select(\n",
    "        col('competition'),\n",
    "        col('filters'),\n",
    "        col('resultSet'),\n",
    "        explode('matches').alias('match')\n",
    "    )\n",
    "\n",
    "def flaten_structs(df: DataFrame) -> DataFrame:\n",
    "    return df.select(\n",
    "        col('competition.code').alias('competition_code'),\n",
    "        col('competition.emblem'),\n",
    "        col('competition.id').alias('competition_id'),\n",
    "        col('competition.name').alias('competition_name'),\n",
    "        col('competition.type'),\n",
    "        col('filters.season'),\n",
    "        get('filters.status', 0).alias('status'),\n",
    "        col('resultSet.count'),\n",
    "        col('resultSet.first'),\n",
    "        col('resultSet.last'),\n",
    "        col('resultSet.played'),\n",
    "        col('match.area.code').alias('area_code'),\n",
    "        col('match.area.flag'),\n",
    "        col('match.area.id').alias('area_id'),\n",
    "        col('match.area.name').alias('area_name'),\n",
    "        col('match.awayTeam_id'),\n",
    "        col('match.group'),\n",
    "        col('match.homeTeam_id'),\n",
    "        col('match.id').alias('match_id'),\n",
    "        col('match.lastUpdated'),\n",
    "        col('match.matchday'),\n",
    "        get('match.referees', 0).getItem('id').alias('referee_id'),\n",
    "        get('match.referees', 0).getItem('name').alias('referee_name'),\n",
    "        get('match.referees', 0).getItem('nationality').alias('referee_nationality'),\n",
    "        get('match.referees', 0).getItem('type').alias('referee_type'),\n",
    "        col('match.score.fullTime.away').alias('fullTime_away'),\n",
    "        col('match.score.fullTime.home').alias('fullTime_home'),\n",
    "        col('match.score.halfTime.away').alias('halfTime_away'),\n",
    "        col('match.score.halfTime.home').alias('halfTime_home'),\n",
    "        col('match.score.winner').alias('score_winner'),\n",
    "        col('match.season.currentMatchday'),\n",
    "        col('match.season.id').alias('season_id'),\n",
    "        col('match.season.startDate'),\n",
    "        col('match.season.endDate'),\n",
    "        col('match.season.winner').alias('season_winner'),\n",
    "        col('match.stage'),\n",
    "        col('match.utcDate')\n",
    "    )\n",
    "\n",
    "def type_casting(df: DataFrame) -> DataFrame:\n",
    "    transformations = {}\n",
    "    date_cols = ['first', 'last', 'startDate', 'endDate']\n",
    "\n",
    "    for col_name in date_cols:\n",
    "        transformations[col_name] = to_date(col(col_name), 'yyyy-MM-dd')\n",
    "    transformations['utcDate'] = to_timestamp(col('utcDate'), \"yyyy-MM-dd'T'HH:mm:ssX\")\n",
    "\n",
    "    return df.withColumns(transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "155c160d-276e-427a-a53d-d453c4ac00a1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define cleaning transformations"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.labs.dqx import check_funcs\n",
    "from databricks.labs.dqx.engine import DQEngine\n",
    "from databricks.labs.dqx.rule import DQRowRule, DQDatasetRule, DQForEachColRule\n",
    "from databricks.labs.dqx.config import InputConfig, OutputConfig\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "def data_quality_checks(df: DataFrame) -> DataFrame:\n",
    "    dq_engine = DQEngine(WorkspaceClient())\n",
    "\n",
    "    checks = [\n",
    "        DQDatasetRule(\n",
    "            name='Check match_id uniqueness',\n",
    "            columns=['match_id'],\n",
    "            check_func=check_funcs.is_unique,\n",
    "            criticality='error'\n",
    "        ),\n",
    "        *DQForEachColRule(\n",
    "            name='Check that match_id, awayTeam_id & homeTeam_id are not null',\n",
    "            check_func=check_funcs.is_not_null_and_not_empty,\n",
    "            criticality='error',\n",
    "            columns=['match_id', 'awayTeam_id', 'homeTeam_id']\n",
    "        ).get_rules(),\n",
    "        *DQForEachColRule(\n",
    "            name='Check that date fields are not in future',\n",
    "            check_func=check_funcs.is_not_in_future,\n",
    "            criticality='error',\n",
    "            columns=['first', 'startDate', 'utcDate']\n",
    "        ).get_rules(),\n",
    "        DQRowRule(\n",
    "            name='Check status',\n",
    "            check_func=check_funcs.is_equal_to,\n",
    "            criticality='warn',\n",
    "            column='status',\n",
    "            check_func_kwargs={\"value\": lit(\"FINISHED\")}\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    valid_df, quarantined_df = dq_engine.apply_checks_and_split(df, checks)\n",
    "    return valid_df\n",
    "\n",
    "def select_fields(df: DataFrame) -> DataFrame:\n",
    "    return df.select(\n",
    "        col('match_id'),\n",
    "        col('awayTeam_id'),\n",
    "        col('homeTeam_id'),\n",
    "        col('halfTime_home'),\n",
    "        col('halfTime_away'),\n",
    "        col('fullTime_home'),\n",
    "        col('fullTime_away'),\n",
    "        col('score_winner'),\n",
    "        col('matchday'),\n",
    "        col('utcDate'),\n",
    "        col('referee_id'),\n",
    "        col('referee_name'),\n",
    "        col('lastUpdated')\n",
    "    )\n",
    "\n",
    "def normalize_col_names(df: DataFrame) -> DataFrame:\n",
    "    return df.withColumnsRenamed({\n",
    "        'awayTeam_id': 'away_team_id',\n",
    "        'homeTeam_id': 'home_team_id',\n",
    "        'halfTime_home': 'half_time_home',\n",
    "        'halfTime_away': 'half_time_away',\n",
    "        'fullTime_home': 'full_time_home',\n",
    "        'fullTime_away': 'full_time_away',\n",
    "        'score_winner': 'winner',\n",
    "        'utcDate': 'match_timpestamp',\n",
    "        'lastUpdated': 'last_updated'\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d4620b6-5195-4939-ad7c-efb69d6b73ff",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1758909353918}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Apply transformations"
    }
   },
   "outputs": [],
   "source": [
    "stg_matches_df = (\n",
    "    raw_matches_df\n",
    "    .transform(explode_arr)\n",
    "    .transform(flaten_structs)\n",
    "    .transform(type_casting)\n",
    "    .transform(data_quality_checks)\n",
    "    .transform(select_fields)\n",
    "    .transform(normalize_col_names)\n",
    ")\n",
    "\n",
    "stg_matches_df.printSchema()\n",
    "display(stg_matches_df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e84753b-8f4b-4b82-bb2a-5ca53783b70d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Incremental Upsert"
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def incremental_upsert(dest_table: str, df: DataFrame, unique_key: str, updated_at: str, full_refresh=False):\n",
    "    if not spark.catalog.tableExists(dest_table) or full_refresh:\n",
    "        (\n",
    "            df\n",
    "            .write\n",
    "            .format('delta')\n",
    "            .mode('overwrite')\n",
    "            .option('overwriteSchema', 'true')\n",
    "            .saveAsTable(dest_table)\n",
    "        )\n",
    "    else:\n",
    "        last_max = (\n",
    "            spark.table(dest_table)\n",
    "                .agg(F.max(updated_at).alias('max_ts'))\n",
    "                .collect()[0]['max_ts']\n",
    "        )\n",
    "\n",
    "        incr_df = df.filter(F.col(updated_at) > last_max)\n",
    "\n",
    "        if not incr_df.isEmpty():\n",
    "            delta_table = DeltaTable.forName(spark, dest_table)\n",
    "            (\n",
    "                delta_table.alias('t')\n",
    "                    .merge(\n",
    "                        source=incr_df.alias('s'),\n",
    "                        condition=f's.{unique_key} = t.{unique_key}'\n",
    "                    )\n",
    "                    .whenMatchedUpdateAll()\n",
    "                    .whenNotMatchedInsertAll()\n",
    "                    .execute()\n",
    "            )\n",
    "\n",
    "dest_table = 'default.stg_matches'\n",
    "incremental_upsert(dest_table, stg_matches_df, 'match_id', 'last_updated')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_matches",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
